{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Group Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to `pip install` all required libs from `requirements.txt` using `pip install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ProgSnap2 import PS2\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep & Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "semester = 'S19'\n",
    "BASE_PATH = os.path.join('data', semester)\n",
    "TRAIN_PATH = os.path.join(BASE_PATH, 'Train')\n",
    "TEST_PATH = os.path.join(BASE_PATH, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SubjectID</th>\n",
       "      <th>AssignmentID</th>\n",
       "      <th>ProblemID</th>\n",
       "      <th>Attempts</th>\n",
       "      <th>CorrectEventually</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>04c32d4d95425f73b3a1d6502aed4d48</td>\n",
       "      <td>439.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>04c32d4d95425f73b3a1d6502aed4d48</td>\n",
       "      <td>439.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>04c32d4d95425f73b3a1d6502aed4d48</td>\n",
       "      <td>439.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04c32d4d95425f73b3a1d6502aed4d48</td>\n",
       "      <td>439.0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>04c32d4d95425f73b3a1d6502aed4d48</td>\n",
       "      <td>439.0</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6637</th>\n",
       "      <td>ffb72475a81de0e95b910ffad039f5c2</td>\n",
       "      <td>492.0</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6638</th>\n",
       "      <td>ffb72475a81de0e95b910ffad039f5c2</td>\n",
       "      <td>492.0</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6639</th>\n",
       "      <td>ffb72475a81de0e95b910ffad039f5c2</td>\n",
       "      <td>492.0</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6640</th>\n",
       "      <td>ffb72475a81de0e95b910ffad039f5c2</td>\n",
       "      <td>492.0</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6641</th>\n",
       "      <td>ffb72475a81de0e95b910ffad039f5c2</td>\n",
       "      <td>492.0</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6642 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             SubjectID  AssignmentID  ProblemID  Attempts  \\\n",
       "0     04c32d4d95425f73b3a1d6502aed4d48         439.0          1         1   \n",
       "1     04c32d4d95425f73b3a1d6502aed4d48         439.0          3         2   \n",
       "2     04c32d4d95425f73b3a1d6502aed4d48         439.0          5         3   \n",
       "3     04c32d4d95425f73b3a1d6502aed4d48         439.0         12         1   \n",
       "4     04c32d4d95425f73b3a1d6502aed4d48         439.0         13         2   \n",
       "...                                ...           ...        ...       ...   \n",
       "6637  ffb72475a81de0e95b910ffad039f5c2         492.0         37         1   \n",
       "6638  ffb72475a81de0e95b910ffad039f5c2         492.0         38         1   \n",
       "6639  ffb72475a81de0e95b910ffad039f5c2         492.0         39         1   \n",
       "6640  ffb72475a81de0e95b910ffad039f5c2         492.0         40         3   \n",
       "6641  ffb72475a81de0e95b910ffad039f5c2         492.0        128         1   \n",
       "\n",
       "      CorrectEventually  Label  \n",
       "0                  True   True  \n",
       "1                  True   True  \n",
       "2                  True   True  \n",
       "3                  True   True  \n",
       "4                  True   True  \n",
       "...                 ...    ...  \n",
       "6637               True   True  \n",
       "6638               True   True  \n",
       "6639               True   True  \n",
       "6640               True   True  \n",
       "6641               True   True  \n",
       "\n",
       "[6642 rows x 6 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_train = pd.read_csv(os.path.join(TRAIN_PATH, 'early.csv'))\n",
    "early_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SubjectID</th>\n",
       "      <th>AssignmentID</th>\n",
       "      <th>ProblemID</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>04c32d4d95425f73b3a1d6502aed4d48</td>\n",
       "      <td>494.0</td>\n",
       "      <td>41</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>04c32d4d95425f73b3a1d6502aed4d48</td>\n",
       "      <td>494.0</td>\n",
       "      <td>43</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>04c32d4d95425f73b3a1d6502aed4d48</td>\n",
       "      <td>494.0</td>\n",
       "      <td>44</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04c32d4d95425f73b3a1d6502aed4d48</td>\n",
       "      <td>494.0</td>\n",
       "      <td>46</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>04c32d4d95425f73b3a1d6502aed4d48</td>\n",
       "      <td>494.0</td>\n",
       "      <td>49</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4196</th>\n",
       "      <td>ffb72475a81de0e95b910ffad039f5c2</td>\n",
       "      <td>502.0</td>\n",
       "      <td>64</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4197</th>\n",
       "      <td>ffb72475a81de0e95b910ffad039f5c2</td>\n",
       "      <td>502.0</td>\n",
       "      <td>70</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4198</th>\n",
       "      <td>ffb72475a81de0e95b910ffad039f5c2</td>\n",
       "      <td>502.0</td>\n",
       "      <td>71</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4199</th>\n",
       "      <td>ffb72475a81de0e95b910ffad039f5c2</td>\n",
       "      <td>502.0</td>\n",
       "      <td>112</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4200</th>\n",
       "      <td>ffb72475a81de0e95b910ffad039f5c2</td>\n",
       "      <td>502.0</td>\n",
       "      <td>118</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4201 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             SubjectID  AssignmentID  ProblemID  Label\n",
       "0     04c32d4d95425f73b3a1d6502aed4d48         494.0         41  False\n",
       "1     04c32d4d95425f73b3a1d6502aed4d48         494.0         43   True\n",
       "2     04c32d4d95425f73b3a1d6502aed4d48         494.0         44   True\n",
       "3     04c32d4d95425f73b3a1d6502aed4d48         494.0         46   True\n",
       "4     04c32d4d95425f73b3a1d6502aed4d48         494.0         49   True\n",
       "...                                ...           ...        ...    ...\n",
       "4196  ffb72475a81de0e95b910ffad039f5c2         502.0         64   True\n",
       "4197  ffb72475a81de0e95b910ffad039f5c2         502.0         70   True\n",
       "4198  ffb72475a81de0e95b910ffad039f5c2         502.0         71   True\n",
       "4199  ffb72475a81de0e95b910ffad039f5c2         502.0        112   True\n",
       "4200  ffb72475a81de0e95b910ffad039f5c2         502.0        118   True\n",
       "\n",
       "[4201 rows x 4 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "late_train = pd.read_csv(os.path.join(TRAIN_PATH, 'late.csv'))\n",
    "late_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4201, 3), (4201,))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_base = late_train.copy().drop('Label', axis=1)\n",
    "y_train = late_train['Label'].values\n",
    "X_train_base.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "problem_encoder = OneHotEncoder().fit(X_train_base[PS2.ProblemID].values.reshape(-1, 1))\n",
    "problem_encoder.transform(X_train_base[PS2.ProblemID].values.reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_instance_features(instance, early_df):\n",
    "    instance = instance.copy()\n",
    "    subject_id = instance[PS2.SubjectID]\n",
    "    early_problems = early_df[early_df[PS2.SubjectID] == subject_id]\n",
    "    # Extract very naive features about the student\n",
    "    # (without respect to the problem bring predicted)\n",
    "    # Number of early problems attempted\n",
    "    instance['ProblemsAttempted'] = early_problems.shape[0]\n",
    "    # Percentage of early problems gotten correct eventually\n",
    "    instance['PercCorrectEventually'] = np.mean(early_problems['CorrectEventually'])\n",
    "    # Median attempts made on early problems\n",
    "    instance['MedAttempts'] = np.median(early_problems['Attempts'])\n",
    "    # Max attempts made on early problems\n",
    "    instance['MaxAttempts'] = np.max(early_problems['Attempts'])\n",
    "    # Percentage of problems gotten correct on the first try\n",
    "    instance['PercCorrectFirstTry'] = np.mean(early_problems['Attempts'] == 1)\n",
    "    df_generatedFeatures = pd.read_csv(os.path.join('Code/newEarlyTrain.csv'))\n",
    "    df_generatedFeatures_problems = df_generatedFeatures[df_generatedFeatures[PS2.SubjectID] == subject_id]\n",
    "    # print(len(df_generatedFeatures_problems))\n",
    "    instance['PercSubjectSyntaxErrors'] = np.median(df_generatedFeatures_problems['pSubjectSyntaxError'])\n",
    "    instance['PercSubjectSemanticErrors'] = np.median(df_generatedFeatures_problems['pSubjectSemanticError'])\n",
    "    instance['meanLabels'] = np.mean(early_problems['Label'])\n",
    "    instance = instance.drop('SubjectID')\n",
    "    return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AssignmentID                    494.0\n",
      "ProblemID                          43\n",
      "ProblemsAttempted                  30\n",
      "PercCorrectEventually             1.0\n",
      "MedAttempts                       6.5\n",
      "MaxAttempts                        45\n",
      "PercCorrectFirstTry          0.166667\n",
      "PercSubjectSyntaxErrors         0.667\n",
      "PercSubjectSemanticErrors         0.8\n",
      "meanLabels                   0.433333\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(extract_instance_features(X_train_base.iloc[1], early_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AssignmentID                    494.0\n",
      "ProblemID                          44\n",
      "ProblemsAttempted                  30\n",
      "PercCorrectEventually             1.0\n",
      "MedAttempts                       6.5\n",
      "MaxAttempts                        45\n",
      "PercCorrectFirstTry          0.166667\n",
      "PercSubjectSyntaxErrors         0.667\n",
      "PercSubjectSemanticErrors         0.8\n",
      "meanLabels                   0.433333\n",
      "Name: 2, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(extract_instance_features(X_train_base.iloc[2], early_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(X, early_df, scaler, is_train):\n",
    "    # First extract performance features for each row\n",
    "    features = X.apply(lambda instance: extract_instance_features(instance, early_df), axis=1)\n",
    "    # Then one-hot encode the problem_id and append it\n",
    "    # problem_ids = problem_encoder.transform(features[PS2.ProblemID].values.reshape(-1, 1)).toarray()\n",
    "    problem_ids = features[PS2.ProblemID].values.reshape(-1, 1)\n",
    "    # Then get rid of nominal features\n",
    "    features.drop([PS2.AssignmentID, PS2.ProblemID], axis=1, inplace=True)\n",
    "    # Then scale the continuous features, fitting the scaler if this is training\n",
    "    if is_train:\n",
    "        scaler.fit(features)\n",
    "    features = scaler.transform(features)\n",
    "    # Return continuous and one-hot features together\n",
    "    return np.concatenate([features, problem_ids], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4201, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.51751812,  0.58371895,  1.76922077,  1.70602676, -0.89569333,\n",
       "         0.65105279,  1.62095701, -1.41126789, 41.        ],\n",
       "       [ 0.51751812,  0.58371895,  1.76922077,  1.70602676, -0.89569333,\n",
       "         0.65105279,  1.62095701, -1.41126789, 43.        ]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = extract_features(X_train_base, early_train, scaler, True)\n",
    "\n",
    "print(X_train.shape)\n",
    "X_train[:2, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Code/LateTrainAllFeatures.pickle', 'wb') as handle:\n",
    "    pickle.dump(X_train, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.51751812,   0.58371895,   1.76922077, ...,   1.62095701,\n",
       "         -1.41126789,  41.        ],\n",
       "       [  0.51751812,   0.58371895,   1.76922077, ...,   1.62095701,\n",
       "         -1.41126789,  43.        ],\n",
       "       [  0.51751812,   0.58371895,   1.76922077, ...,   1.62095701,\n",
       "         -1.41126789,  44.        ],\n",
       "       ...,\n",
       "       [ -1.52965101,   0.58371895,  -0.48646859, ...,  -1.12815153,\n",
       "          0.36655411,  71.        ],\n",
       "       [ -1.52965101,   0.58371895,  -0.48646859, ...,  -1.12815153,\n",
       "          0.36655411, 112.        ],\n",
       "       [ -1.52965101,   0.58371895,  -0.48646859, ...,  -1.12815153,\n",
       "          0.36655411, 118.        ]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "late_train = pd.read_csv(os.path.join(TRAIN_PATH, 'late.csv'))\n",
    "\n",
    "\n",
    "with open('Code/LateTrainAllFeatures.pickle', 'rb') as handle:\n",
    "    X_Train = pickle.load(handle)\n",
    "\n",
    "TrainLateProblemList = pd.read_csv('Code/LateTrainProblemList.csv')\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perProblemFeatures(X_train, prob_id):\n",
    "    # print('Problem ID: ', prob_id)\n",
    "    n_previously_generated_features = 8\n",
    "    subjectFeatures = []\n",
    "    for i in range(len(X_train)):\n",
    "        if X_train[i, n_previously_generated_features] == prob_id:\n",
    "            subjectFeatures.append(X_train[i, :8]) # Try with different features from 5-8 (First 5 from Naive Model)\n",
    "    return subjectFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem_X_Train = []\n",
    "sum = 0\n",
    "\n",
    "for i in range(20):\n",
    "    # print(TrainLateProblemList['ProblemID'].iloc[i])\n",
    "    problem_X_Train.append(perProblemFeatures(X_Train, TrainLateProblemList['ProblemID'].iloc[i]))\n",
    "    sum += len(problem_X_Train[i])\n",
    "    # print(len(problem_X_Train[i]))\n",
    "\n",
    "len(problem_X_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_Train = []\n",
    "\n",
    "for i in range(len(TrainLateProblemList)):\n",
    "    y_TrainPerProblem = []\n",
    "    for j in range(len(late_train)):\n",
    "        if late_train['ProblemID'].iloc[j] == TrainLateProblemList['ProblemID'].iloc[i]:\n",
    "            y_TrainPerProblem.append(late_train['Label'].iloc[j])\n",
    "    y_Train.append(y_TrainPerProblem)\n",
    "\n",
    "len(y_Train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean model f1 score:  0.5289463730389105\n",
      "Mean cv f1 score:  0.5292138201423396\n",
      "AUC Model:  0.5654459542637073\n",
      "AUC CV:  0.7249301878676879\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "macrof1_model = []\n",
    "macrof1_cv = []\n",
    "AUC_model = []\n",
    "AUC_cv = []\n",
    "\n",
    "for l in range(len(y_Train)):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(problem_X_Train[l], y_Train[l], test_size=0.2, random_state=42)\n",
    "    model = LogisticRegressionCV()\n",
    "    model.fit(X_train, Y_train)\n",
    "    train_predictions = model.predict(X_test)\n",
    "    # print(classification_report(Y_test, train_predictions))\n",
    "    # print('AUC: ' + str(roc_auc_score(Y_test, train_predictions)))\n",
    "    # print('Macro F1: ' + str(f1_score(Y_test, train_predictions, average='macro')))\n",
    "    macrof1_model.append((f1_score(Y_test, train_predictions, average='macro')))\n",
    "    AUC_model.append(roc_auc_score(Y_test, train_predictions))\n",
    "    cv_results = cross_validate(model, X_train, Y_train, cv=10, scoring=['accuracy', 'f1_macro', 'roc_auc'])\n",
    "    # print(f'Accuracy: {np.mean(cv_results[\"test_accuracy\"])}')\n",
    "    # print(f'AUC: {np.mean(cv_results[\"test_roc_auc\"])}')\n",
    "    # print(f'Macro F1: {np.mean(cv_results[\"test_f1_macro\"])}')\n",
    "    macrof1_cv.append(np.mean(cv_results[\"test_f1_macro\"]))\n",
    "    AUC_cv.append(np.mean(cv_results[\"test_roc_auc\"]))\n",
    "\n",
    "print('Mean model f1 score: ', np.mean(macrof1_model))\n",
    "print(\"Mean cv f1 score: \", np.mean(macrof1_cv))\n",
    "print('AUC Model: ', np.mean(AUC_model))\n",
    "print('AUC CV: ', np.mean(AUC_cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean model f1 score:  0.5779232081209053\n",
      "Mean cv f1 score:  0.556204217956733\n",
      "AUC Model:  0.5866205216439934\n",
      "AUC CV:  0.7013907967032968\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "macrof1_model = []\n",
    "macrof1_cv = []\n",
    "AUC_model = []\n",
    "AUC_cv = []\n",
    "\n",
    "for l in range(len(y_Train)):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(problem_X_Train[l], y_Train[l], test_size=0.2, random_state=42)\n",
    "    model = RandomForestClassifier(n_estimators=500, max_leaf_nodes=64, n_jobs=-1)\n",
    "    model.fit(X_train, Y_train)\n",
    "    train_predictions = model.predict(X_test)\n",
    "    # print(classification_report(Y_test, train_predictions))\n",
    "    # print('AUC: ' + str(roc_auc_score(Y_test, train_predictions)))\n",
    "    # print('Macro F1: ' + str(f1_score(Y_test, train_predictions, average='macro')))\n",
    "    macrof1_model.append((f1_score(Y_test, train_predictions, average='macro')))\n",
    "    AUC_model.append(roc_auc_score(Y_test, train_predictions))\n",
    "    cv_results = cross_validate(model, X_train, Y_train, cv=10, scoring=['accuracy', 'f1_macro', 'roc_auc'])\n",
    "    # print(f'Accuracy: {np.mean(cv_results[\"test_accuracy\"])}')\n",
    "    # print(f'AUC: {np.mean(cv_results[\"test_roc_auc\"])}')\n",
    "    # print(f'Macro F1: {np.mean(cv_results[\"test_f1_macro\"])}')\n",
    "    macrof1_cv.append(np.mean(cv_results[\"test_f1_macro\"]))\n",
    "    AUC_cv.append(np.mean(cv_results[\"test_roc_auc\"]))\n",
    "\n",
    "print('Mean model f1 score: ', np.mean(macrof1_model))\n",
    "print(\"Mean cv f1 score: \", np.mean(macrof1_cv))\n",
    "print('AUC Model: ', np.mean(AUC_model))\n",
    "print('AUC CV: ', np.mean(AUC_cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial RBF SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean model f1 score:  0.42105901484035435\n",
      "Mean cv f1 score:  0.4259118937244007\n",
      "AUC Model:  0.5\n",
      "AUC CV:  0.6081144827394828\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "macrof1_model = []\n",
    "macrof1_cv = []\n",
    "AUC_model = []\n",
    "AUC_cv = []\n",
    "\n",
    "for l in range(len(y_Train)):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(problem_X_Train[l], y_Train[l], test_size=0.2, random_state=42)\n",
    "    model = svm.SVC(kernel='rbf', gamma=5, C=0.001)\n",
    "    model.fit(X_train, Y_train)\n",
    "    train_predictions = model.predict(X_test)\n",
    "    # print(classification_report(Y_test, train_predictions))\n",
    "    # print('AUC: ' + str(roc_auc_score(Y_test, train_predictions)))\n",
    "    # print('Macro F1: ' + str(f1_score(Y_test, train_predictions, average='macro')))\n",
    "    macrof1_model.append((f1_score(Y_test, train_predictions, average='macro')))\n",
    "    AUC_model.append(roc_auc_score(Y_test, train_predictions))\n",
    "    cv_results = cross_validate(model, X_train, Y_train, cv=10, scoring=['accuracy', 'f1_macro', 'roc_auc'])\n",
    "    # print(f'Accuracy: {np.mean(cv_results[\"test_accuracy\"])}')\n",
    "    # print(f'AUC: {np.mean(cv_results[\"test_roc_auc\"])}')\n",
    "    # print(f'Macro F1: {np.mean(cv_results[\"test_f1_macro\"])}')\n",
    "    macrof1_cv.append(np.mean(cv_results[\"test_f1_macro\"]))\n",
    "    AUC_cv.append(np.mean(cv_results[\"test_roc_auc\"]))\n",
    "\n",
    "print('Mean model f1 score: ', np.mean(macrof1_model))\n",
    "print(\"Mean cv f1 score: \", np.mean(macrof1_cv))\n",
    "print('AUC Model: ', np.mean(AUC_model))\n",
    "print('AUC CV: ', np.mean(AUC_cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['For', 'Math+-*/', 'ArrayIndex']\n",
      "['If/Else', 'For', 'LogicCompareNum', 'ArrayIndex']\n",
      "['If/Else', 'For', 'Math+-*/', 'Math%', 'LogicCompareNum', 'ArrayIndex']\n",
      "['If/Else', 'For', 'LogicAndNotOr', 'LogicCompareNum', 'ArrayIndex']\n",
      "['If/Else', 'For', 'Math+-*/', 'LogicCompareNum', 'ArrayIndex']\n",
      "['If/Else', 'For', 'Math+-*/', 'LogicAndNotOr', 'LogicCompareNum', 'ArrayIndex']\n",
      "['If/Else', 'For', 'NestedFor', 'Math%', 'LogicAndNotOr', 'LogicCompareNum', 'ArrayIndex']\n",
      "['If/Else', 'For', 'NestedFor', 'LogicAndNotOr', 'LogicCompareNum', 'ArrayIndex']\n",
      "['If/Else', 'For', 'Math+-*/', 'LogicAndNotOr', 'LogicCompareNum', 'ArrayIndex']\n",
      "['If/Else', 'For', 'LogicAndNotOr', 'LogicCompareNum', 'ArrayIndex']\n",
      "['If/Else', 'For', 'Math+-*/', 'LogicAndNotOr', 'LogicCompareNum', 'ArrayIndex']\n",
      "['If/Else', 'For', 'LogicCompareNum', 'ArrayIndex']\n",
      "['If/Else', 'For', 'Math%', 'LogicCompareNum', 'ArrayIndex']\n",
      "['If/Else', 'For', 'Math+-*/', 'LogicAndNotOr', 'LogicCompareNum', 'ArrayIndex']\n",
      "['For', 'ArrayIndex']\n",
      "['If/Else', 'For', 'Math%', 'LogicCompareNum', 'StringFormat', 'StringEqual', 'ArrayIndex']\n",
      "['If/Else', 'For', 'LogicAndNotOr', 'LogicCompareNum', 'ArrayIndex']\n",
      "['If/Else', 'For', 'Math+-*/', 'LogicCompareNum', 'ArrayIndex']\n",
      "['For', 'NestedFor', 'Math+-*/', 'ArrayIndex']\n",
      "['For', 'ArrayIndex']\n",
      "[['For', 'Math+-*/', 'ArrayIndex'], ['If/Else', 'For', 'LogicCompareNum', 'ArrayIndex'], ['If/Else', 'For', 'Math+-*/', 'Math%', 'LogicCompareNum', 'ArrayIndex'], ['If/Else', 'For', 'LogicAndNotOr', 'LogicCompareNum', 'ArrayIndex'], ['If/Else', 'For', 'Math+-*/', 'LogicCompareNum', 'ArrayIndex'], ['If/Else', 'For', 'Math+-*/', 'LogicAndNotOr', 'LogicCompareNum', 'ArrayIndex'], ['If/Else', 'For', 'NestedFor', 'Math%', 'LogicAndNotOr', 'LogicCompareNum', 'ArrayIndex'], ['If/Else', 'For', 'NestedFor', 'LogicAndNotOr', 'LogicCompareNum', 'ArrayIndex'], ['If/Else', 'For', 'Math+-*/', 'LogicAndNotOr', 'LogicCompareNum', 'ArrayIndex'], ['If/Else', 'For', 'LogicAndNotOr', 'LogicCompareNum', 'ArrayIndex'], ['If/Else', 'For', 'Math+-*/', 'LogicAndNotOr', 'LogicCompareNum', 'ArrayIndex'], ['If/Else', 'For', 'LogicCompareNum', 'ArrayIndex'], ['If/Else', 'For', 'Math%', 'LogicCompareNum', 'ArrayIndex'], ['If/Else', 'For', 'Math+-*/', 'LogicAndNotOr', 'LogicCompareNum', 'ArrayIndex'], ['For', 'ArrayIndex'], ['If/Else', 'For', 'Math%', 'LogicCompareNum', 'StringFormat', 'StringEqual', 'ArrayIndex'], ['If/Else', 'For', 'LogicAndNotOr', 'LogicCompareNum', 'ArrayIndex'], ['If/Else', 'For', 'Math+-*/', 'LogicCompareNum', 'ArrayIndex'], ['For', 'NestedFor', 'Math+-*/', 'ArrayIndex'], ['For', 'ArrayIndex']]\n"
     ]
    }
   ],
   "source": [
    "def extractSkills():\n",
    "    df = pd.read_csv('Code/Approach2/ConceptsUsed.csv')\n",
    "    last_skills = []\n",
    "    final_skills = []\n",
    "\n",
    "    for i in range(30, len(df)):\n",
    "        for column, row in df.iteritems():\n",
    "            if df[column].iloc[i] == 1:\n",
    "                last_skills.append(column)\n",
    "        print(last_skills)\n",
    "        final_skills.append(last_skills)\n",
    "        last_skills = []\n",
    "    print(final_skills)\n",
    "\n",
    "    with open('Code/Approach2/LateSkills.pickle', 'wb') as handle:\n",
    "        pickle.dump(final_skills, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "extractSkills()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI Project",
   "language": "python",
   "name": "aiproj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
